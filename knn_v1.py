# -*- coding: utf-8 -*-
"""KNN v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aPFBGRX4VliC3uw7n1wTRGOPrMyE0_Pb

# DETEKSI BERITA HOAX  BBM MENGGUNAKAN METODE K-NEAREST NEIGHBOR

## Install Library Tambahan
"""

!pip install Sastrawi
!pip install -U scikit-learn
# !python -m pip install --upgrade --user scikit-learn

"""## Import Library"""

import pandas as pd
import nltk

from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
nltk.download('punkt')

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, ConfusionMatrixDisplay

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

"""## Preprocessing Data"""

# Memasukan datset berita hoax dari hasil scraping di website kominfo
data_hoax = pd.read_csv('data berita kominfo.csv')
data_hoax.head()

# Menambahkan label hoax
data_hoax['label'] = 'hoax'
data_hoax.head()

# Memasukan datset berita bukan hoax dari hasil scraping di website kompas
data_bukan_hoax = pd.read_csv('data berita kompas.csv')
data_bukan_hoax.head()

# Menambahkan label hoax
data_bukan_hoax['label'] = 'bukan hoax'
data_bukan_hoax.head()

# Menggabungkan kedua dataset menjadi satu
data = pd.concat([data_hoax, data_bukan_hoax], ignore_index=True)
data.head(1000)

# data.to_csv('dataset berita hoax dan bukan haox.csv', index=True)

#Menampilkan Jumlah baris dan kolom
data.shape

# Mengganti nama pada kolom "Sumber"
data['Sumber'] = data['Sumber'].replace('https://www.kominfo.go.id', 'KOMINFO')
data['Sumber'] = data['Sumber'].replace('https://www.google.com', 'KOMPAS')
data.head(1000)

# Membuang Kolom yang tidak di perlukan di antaranya (Unnamed: 0, Teks, Rangkuman, dan Penulis)
data.drop('Unnamed: 0', axis=1, inplace=True)
data.drop('Teks', axis=1, inplace=True)
data.drop('Rangkuman', axis=1, inplace=True)
data.drop('Penulis', axis=1, inplace=True)

data.head(1000)

"""## Data Cleaning"""

# Menghapus data yang terduplikat berdasarkan kolom 'Judul'
# data = data.drop_duplicates(subset='Judul')

# Menghapus data yang terduplikat berdasarkan kolom 'Judul'
data = data.groupby('Judul').first().reset_index()

# Menghapus kata "[DISINFORMASI]" dan "[HOAKS]" pada kolom "Judul"
data['Judul'] = data['Judul'].str.replace('\[DISINFORMASI\]', '')
data['Judul'] = data['Judul'].str.replace('\[HOAKS\]', '')
data['Judul'] = data['Judul'].str.replace('Halaman all', '')

data

"""## Case Folding"""

# Melakukan case folding pada teks
data['Judul'] = data['Judul'].str.lower()

data

"""## Filtering"""

# Menghapus stopwords pada teks
stop_words = set(stopwords.words('indonesian'))

# Menyimpan stopwords dalam set
data['Judul'] = data['Judul'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
data

"""## Tokenizing"""

# Melakukan tokenisasi pada teks tweet
data['Judul'] = data['Judul'].apply(lambda x: word_tokenize(x))
data

"""## Stemming"""

stemmer = StemmerFactory().create_stemmer()
data['Judul'] = data['Judul'].apply(lambda x: [stemmer.stem(word) for word in x])

data

# Menggabungkan token-token menjadi kalimat-kalimat kembali
data['Judul'] = data['Judul'].apply(' '.join)

# Menggabungkan semua teks menjadi satu teks besar
all_text = ' '.join(data['Judul'].values)

"""## TF-IDF"""

# Inisialisasi objek TfidfVectorizer
vectorizer = TfidfVectorizer()

# Mengubah teks menjadi vektor TF-IDF
tfidf_matrix = vectorizer.fit_transform(data['Judul'])

# Pisahkan fitur (TF-IDF) dan label
X = tfidf_matrix
y = data['label']

"""## Split Data"""

# Bagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Jumlah data latih
jumlah_data_latih = X_train.shape[0]
print("Jumlah data latih:", jumlah_data_latih)

# Jumlah data uji
jumlah_data_uji = X_test.shape[0]
print("Jumlah data uji:", jumlah_data_uji)

"""## Algoritma K-Nearest Neighbor"""

# Inisialisasi model KNN

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(tfidf_matrix, data['label'])

prediksi = knn.predict(tfidf_matrix)
print(prediksi)

# Latih model KNN
knn.fit(X_train, y_train)

# Prediksi label untuk data uji
y_pred = knn.predict(X_test)

# Nilai Aurasi
acc = accuracy_score(y_test, y_pred)
print('Akurasi model KNN: {:.2f}%'.format(acc*100))

"""## Confusion Matrix"""

#confusion matrix
confusion_matrix(y_test, y_pred)

# Plot confusion matrix
labels = ['Hoax', 'Bukan Hoax']
cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, cmap="Blues", fmt="d", xticklabels=labels, yticklabels=labels)
plt.title('Confusion Matrix')
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.show()

"""## Classification Report"""

# Menampilkan hasil evaluasi model
print("Hasil Evaluasi Model KNN:")
print(classification_report(y_test, y_pred))

"""## WordCloud"""

# Membuat word cloud
wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate(all_text)

# Menampilkan word cloud
plt.figure(figsize=(10, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud klasifikasi berita Bahan Bakar Minyak')
plt.show()

"""## Grafik Perbandingan"""

# Hitung jumlah data untuk setiap kategori
count = data['label'].value_counts()

# Buat grafik bar perbandingan
plt.bar(count.index, count.values, color=['blue', 'red'])

# Atur label sumbu x dan y
plt.xlabel('Kategori')
plt.ylabel('Jumlah Data')

# Atur judul grafik
plt.title('Perbandingan Jumlah Data')

# Tampilkan grafik
plt.show()